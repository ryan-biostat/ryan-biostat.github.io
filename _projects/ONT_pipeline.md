---
layout: page
title: Long-Read Sequencing Pipeline
description: customized for Whole Genomes on Oxford Nanopore Long-Read Platforms
img: assets/img/dna_sequencing.jpg
importance: 1
category: Bioinformatics
related_publications: true
---

#### **About**

This project was my first major contribution to the MCW Advanced Genomics Lab. When I started, our capability for Long-Read Sequencing analysis was underdeveloped, so I took a lot of initiative with getting a researched & structured pipeline created. I did a lot of diligence in researching the best-in-class tools for alignment, variant calling, and interpretation.

It was also important to me that this pipeline was automated with clear and concise logging. Knowing that better tools will come around, it was also designed to be modular for the insertion of new tools.

This pipeline has **processed nearly 100 whole genomes** and has contributed to discovering disease relevant mutations providing critial diagnostic answers. 

## Purpose

This pipeline is designed for processing whole genome samples from the Oxford Nanopore Technologies (ONT) PromethION long-read sequencing platform. In our lab, we process these samples on R10.9.1 flowcells, and this workflow interprets the resulting raw sequence data.

The pipeline is managed by a central Python script that feeds the data through a series of specialized bioinformatics software to move from raw data to an annotated list of genetic variants.

## Method

The pipeline consists of the following key steps:

#### 1. **Transfer**

Files are prepared for cross-server transfer. An `rsync` command is used to copy over raw sequencing data from the sequencing instrument to our lab's High-Performance Computing (HPC) cluster, named "maple".

#### 2. **Alignment**

Raw sequences are aligned to both the hg38 (GRCh38) and T2T (Telomere-to-Telomere) human reference genomes using `minimap2`. This process generates a BAM (.bam) file, which serves as the foundational file for subsequent analyses.

#### 3. **CNV Calling**

`ont-spectre`, a Copy Number Variant (CNV) calling tool developed by ONT, is utilized. Although still in development, we use it to create VCF (.vcf) files capable of identifying and annotating large structural variants, specifically those greater than 100kb. This step produces both .vcf and .bed files to aid in analysis.

#### 4. **SV Calling**

`sniffles2` is employed as our primary structural variant (SV) caller. It identifies a range of SVs, including deletions, insertions, translocations, and inversions. The typical size range for variants detected by `sniffles2` is between 10bp and 50kb. The output is a .vcf file.

#### 5. **SNP/Small INDEL Calling**

We use `deepvariant` from Google's DeepMind to call small genetic variants. This software utilizes a deep neural network to analyze the sequencing data. This step calls single nucleotide polymorphisms (SNPs) which are the most common type of genetic variation, representing a change in a single DNA building block (nucleotide), and small INsertions and DELetions (INDELs). This analysis also produces haplotyping information, allowing us to determine which genome strand belongs to which parent.

#### 6. **Phasing**

`whatshap` is used to assign haplotypes to our .bam files. This assignment allows us to determine which allele (parental copy) holds which mutations. This is critical for determining the inheritance mode for potential genetic diseases. `whatshap` achieves this by taking the information produced by `deepvariant` and annotating the .bam file, resulting in a .haplotagged.bam file.

#### 7. **Annotation**

`snpEFF` is used to annotate the small variants (SNPs and INDELs). It enriches the .vcf file by pulling information from multiple public databases, such as ClinVar and dbSNP, to provide aggregate data about known mutations.

We also utilize the **Geneyx** software. This is a proprietary annotation platform that adds richer annotations and formats the datasets in a more contestable, browser-accessible way.

## Discussion

The functional pipeline is an automatic process with precise settings and logging capabilities. A whole genome can be processed in <8hrs using this pipeline. The capabilities of your computing resources will determine how many instances can run in parallel. 

This pipeline is executed via the following command:

{% highlight python linenos %}

# Activate the primary conda environment before running the pipeline

conda activate spectre

python pipeline.py <sample_name> <promethion_data_dir> <maple_output_dir> [--log-level LEVEL]

## Help Output

usage: pipeline.py [-h] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]
sample_name data_dir to_dir

ONT PromethION Data Processing Pipeline.

positional arguments:
sample*name The name of the sample being processed. This should
match a created directory.
data_dir Full path to the directory containing the sample data
on the PromethION server (e.g.,
/data/run_folder/sample_subfolder/).
to_dir Full path to the MAPLE directory where output should
be stored (e.g., /data2/flowcell_10.4.1/mcw_svi*.../).

options:
-h, --help show this help message and exit
--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}
Set the logging level (default: INFO)

{% endhighlight %}

The wrapper script for this process is shown here:

{% highlight python linenos %}

################################################################################

# \_**\_ \_ \_ **\_**** **\_** **\_** **\_** **\_\_** \_ **\_** \_ \_ **\_\_**

# / ** \| \ | |** **| | ** \_ _| ** \| \_\_**| | |_ \_| \ | | \_\_\_\_|

# | | | | \| | | | | |**) || | | |**) | |** | | | | | \| | |**

# | | | | . `|  | |    |  ___/ | | |  ___/|  __| | |      | | | .` | \_\_|

# | |**| | |\ | | | | | _| |_| | | |\_\_**| |\_**\_ _| |_| |\ | |\_\_**

# \_**_/|_| \_| |_| |_| |\_\_\_**|\_| |**\_\_**|**\_\_**|**\_**|\_| \_|**\_\_**|

#

################################################################################

#

# Broeckle Lab Bioinformatics, 2024

#

# This file executes the pipeline for the ONT PromethION sequencing data by

# executing tools at the prompt of the user. These scripts are stored in sub-

# directories of the main directory, and can be edited and updated at a whim.

#

#

################################################################################
################################################################################

import os
import sys
import subprocess
import glob
import argparse
import logging

def end_in_slash(dir):
if not dir.endswith('/'):
dir += '/'
return dir

def main():

        # --- Updated Input Section using argparse---
    parser = argparse.ArgumentParser(description='ONT PromethION Data Processing Pipeline.')
    parser.add_argument('sample_name', type=str,
                        help='The name of the sample being processed. This should match a created directory.')
    parser.add_argument('data_dir', type=str,
                        help='Full path to the directory containing the sample data on the PromethION server (e.g., /data/run_folder/sample_subfolder/).')
    parser.add_argument('to_dir', type=str,
                        help='Full path to the MAPLE directory where output should be stored (e.g., /data2/flowcell_10.4.1/mcw_svi_.../).')
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], help='Set the logging level')

    args = parser.parse_args()

    # Assign parsed arguments to variables, keeping original variable names
    sample_name = args.sample_name
    data_dir = end_in_slash(args.data_dir) # Apply end_in_slash here
    to_dir = end_in_slash(args.to_dir)     # Apply end_in_slash here


        # --- Updating Logging --- #
    log_dir = os.path.join(to_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    log_file_path = os.path.join(log_dir, 'pipeline.log')

    # Configure Logging
    log_level = getattr(logging, args.log_level.upper(), logging.INFO) # Get level from args
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s', # Add timestamp and level
        handlers=[
            logging.FileHandler(log_file_path, mode='w'), # Overwrite log file each run
            logging.StreamHandler(sys.stdout) # Log to console
        ]
    )
    logger = logging.getLogger()

    class StreamToLogger:
        """Fake file-like stream object that redirects writes to a logger instance."""
        def __init__(self, logger_instance, log_level=logging.ERROR):
            self.logger = logger_instance
            self.log_level = log_level
            self.linebuf = ''

        def write(self, buf):
            for line in buf.rstrip().splitlines():
                self.logger.log(self.log_level, line.rstrip())

        def flush(self):
            pass # Required for file-like object interface

    sys.stderr = StreamToLogger(logger, logging.ERROR)

    # Now, we have to do any print() statements to logger.info() statements
    # --- End Logging Setup ---

    ref = '/data/ref/NCBI/GRCh38/Homo_sapiens_assembly38_noALT_noHLA_noDecoy_ERCC.fasta'
    logger.info(f'Reference Genome set to:\n{ref}')

    if not os.path.exists(to_dir):
        logger.error("The MAPLE directory does not exist. Exiting script.")
        sys.exit(1)
    logger.info('The MAPLE directory exists.')
    log_dir = os.path.join(to_dir, 'logs') # Make sure log_dir is defined


    #----------------------
    # Step 1: Prepare data for Transfer and send to Maple
    logger.info(f"Running Step 1: File Transfer for {sample_name} using file_transfer.py")
    transfer_command = ["python", "./pipes/file_transfer.py", sample_name, data_dir, to_dir]
    logger.info("Executing Command:\n" + ' '.join(transfer_command))
    try:
        # Run the script, capture output, check return code
        result_transfer = subprocess.run(transfer_command, check=True, capture_output=True, text=True)
        # Log stdout/stderr from the script for pipeline context (optional, as script logs itself)
        if result_transfer.stdout:
            logger.info(f"file_transfer.py stdout:\n{result_transfer.stdout.strip()}")
        if result_transfer.stderr:
            logger.warning(f"file_transfer.py stderr:\n{result_transfer.stderr.strip()}") # Log stderr as warning
        logger.info("File transfer script executed successfully.")

    except subprocess.CalledProcessError as e:
        logger.error(f"Execution of file_transfer.py failed with exit code {e.returncode}")
        if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
        if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
        sys.exit(f"File transfer step (Step 1) failed with exit code {e.returncode}") # Exit pipeline
    except FileNotFoundError:
         logger.error("Error: 'python' or './pipes/file_transfer.py' not found.")
         sys.exit("Failed to execute file transfer script.")
    except Exception as e:
         logger.exception("An unexpected error occurred during the file transfer step:")
         sys.exit("Unexpected error in file transfer step (Step 1).")

    logger.info("%%%%%%%%%%%%%\nFINISHED STEP 1: DATA TRANSFER \n%%%%%%%%%%%%%")
    #----------------------

    # Step 2: Read Alignment (FASTQs w/ GRCh38)
    # Note: alignment.py checks for the fastq file internally
    logger.info(f"Running Step 2: Read Alignment for {sample_name} using alignment.py")
    alignment_command = ["python", "./pipes/alignment.py", sample_name, to_dir, ref]
    logger.info("Executing Command:\n" + ' '.join(alignment_command))
    try:
        # Run the script, capture output, check return code
        result_alignment = subprocess.run(alignment_command, check=True, capture_output=True, text=True)
        # Log stdout/stderr from the script for pipeline context (optional)
        if result_alignment.stdout:
            logger.info(f"alignment.py stdout:\n{result_alignment.stdout.strip()}")
        if result_alignment.stderr:
            logger.warning(f"alignment.py stderr:\n{result_alignment.stderr.strip()}")
        logger.info("Alignment script executed successfully.")

    except subprocess.CalledProcessError as e:
        logger.error(f"Execution of alignment.py failed with exit code {e.returncode}")
        if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
        if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
        sys.exit(f"Alignment step (Step 2) failed with exit code {e.returncode}") # Exit pipeline
    except FileNotFoundError:
         logger.error("Error: 'python' or './pipes/alignment.py' not found.")
         sys.exit("Failed to execute alignment script.")
    except Exception as e:
         logger.exception("An unexpected error occurred during the alignment step:")
         sys.exit("Unexpected error in alignment step (Step 2).")

    logger.info("%%%%%%%%%%%%%\n FINISHED STEP 2: ALIGNMENT \n%%%%%%%%%%%%%")
    logger.info('Next Step: SNP/INDEL via DeepVariant and Phasing with WhatsHap')

    #----------------------
    # Step 3: SNP/INDEL calling with DeepVariant and Phasing with WhatsHap

    if os.path.exists(os.path.join(to_dir, f"{sample_name}.sorted.bam")):
        logger.info(f"Running Step 3: DeepVariant and WhatsHap for {sample_name}")
        deepvariant_dir = os.path.join(to_dir, "deepvariant")
        bam_file = os.path.join(to_dir, f"{sample_name}.sorted.bam")
        os.makedirs(deepvariant_dir, exist_ok=True)

        # Run DeepVariant
        logger.info("Running DeepVariant using deepvariant.py")
        deepvariant_command = [
            "python", "./pipes/deepvariant.py",
            "--sample", sample_name,
            "--input-dir", to_dir,
            "--output-dir", deepvariant_dir,
            "--ref", ref,
            "--bam", bam_file,
            "--log-base-dir", to_dir,
            "--threads", "64"
        ]
        logger.info("Executing Command:\n" + ' '.join(deepvariant_command))
        try:
            result_deepvariant = subprocess.run(deepvariant_command, check=True, capture_output=True, text=True)
            if result_deepvariant.stdout:
                logger.info(f"deepvariant.py stdout:\n{result_deepvariant.stdout.strip()}")
            if result_deepvariant.stderr:
                logger.warning(f"deepvariant.py stderr:\n{result_deepvariant.stderr.strip()}")
            logger.info("deepvariant.py script executed successfully.")
        except subprocess.CalledProcessError as e:
            logger.error(f"Execution of deepvariant.py failed with exit code {e.returncode}")
            if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
            if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
            sys.exit(f"DeepVariant step failed with exit code {e.returncode}")
        except Exception as e:
            logger.exception("An unexpected error occurred during the DeepVariant step:")
            sys.exit("Unexpected error in DeepVariant step.")

        # Run WhatsHap
        deepvariant_vcf = os.path.join(deepvariant_dir, f"{sample_name}_deepvariant.vcf.gz")
        if os.path.exists(deepvariant_vcf):
            logger.info("Running WhatsHap using whatshap_phase-haplotag.py")
            # --- UPDATED COMMAND TO USE `conda run` ---
            whatshap_command = [
                "conda", "run", "-n", "whatshap-env", "python", "./pipes/whatshap_phase-haplotag.py",
                "--vcf", deepvariant_vcf,
                "--bam", bam_file,
                "--ref", ref,
                "--output-dir", deepvariant_dir,
                "--log-base-dir", to_dir
            ]
            logger.info("Executing Command:\n" + ' '.join(whatshap_command))
            try:
                result_whatshap = subprocess.run(whatshap_command, check=True, capture_output=True, text=True)
                if result_whatshap.stdout:
                    logger.info(f"whatshap_phase-haplotag.py stdout:\n{result_whatshap.stdout.strip()}")
                if result_whatshap.stderr:
                    logger.warning(f"whatshap_phase-haplotag.py stderr:\n{result_whatshap.stderr.strip()}")
                logger.info("whatshap_phase-haplotag.py script executed successfully.")

                # --- ADDITION START ---
                # Index the newly created haplotagged BAM file
                logger.info("Indexing the haplotagged BAM file with samtools.")
                haplotagged_bam = os.path.join(deepvariant_dir, f"{sample_name}_deepvariant_haplotagged.bam")

                if os.path.exists(haplotagged_bam):
                    # Assuming samtools is in the system's PATH. If it's in a specific conda env,
                    # you would prefix the command, e.g., ["conda", "run", "-n", "samtools-env", "samtools", ...]
                    samtools_index_command = ["samtools", "index", haplotagged_bam]
                    logger.info("Executing Command:\n" + ' '.join(samtools_index_command))
                    try:
                        result_samtools = subprocess.run(samtools_index_command, check=True, capture_output=True, text=True)
                        if result_samtools.stdout:
                            logger.info(f"samtools index stdout:\n{result_samtools.stdout.strip()}")
                        if result_samtools.stderr:
                            logger.warning(f"samtools index stderr:\n{result_samtools.stderr.strip()}")
                        logger.info("Successfully indexed the haplotagged BAM file.")
                    except subprocess.CalledProcessError as e:
                        logger.error(f"Execution of samtools index failed with exit code {e.returncode}")
                        if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
                        if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
                        sys.exit(f"samtools index step failed with exit code {e.returncode}")
                    except FileNotFoundError:
                        logger.error("Error: 'samtools' command not found. Make sure it is installed and in your PATH.")
                        sys.exit("Failed to execute samtools index.")
                    except Exception as e:
                        logger.exception("An unexpected error occurred during samtools index.")
                        sys.exit("Unexpected error in samtools index step.")
                else:
                    logger.error(f"Haplotagged BAM file not found for indexing: {haplotagged_bam}")
                    sys.exit("Cannot index non-existent BAM file.")
                # --- ADDITION END ---

            except subprocess.CalledProcessError as e:
                logger.error(f"Execution of whatshap_phase-haplotag.py failed with exit code {e.returncode}")
                if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
                if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
                sys.exit(f"WhatsHap step failed with exit code {e.returncode}")
            except Exception as e:
                logger.exception("An unexpected error occurred during the WhatsHap step:")
                sys.exit("Unexpected error in WhatsHap step.")
        else:
            logger.error(f"DeepVariant output VCF not found: {deepvariant_vcf}. Skipping WhatsHap.")
            sys.exit("DeepVariant output not found.")
    else:
        logger.error(f"Input BAM file not found: {os.path.join(to_dir, f'{sample_name}.sorted.bam')}. Skipping SNP/INDEL calling.")
        sys.exit("Input BAM for SNP/INDEL calling not found.")

    logger.info("%%%%%%%%%%%%%\n FINISHED SNP/INDEL CALLING AND PHASING \n%%%%%%%%%%%%%")
    logger.info('Next Step: CNV via Spectre')

    #----------------------
    # Step 4: Large SV via Spectre

    logger.info(f"Running Spectre step for {sample_name} using spectre.py")
    # --- UPDATED COMMAND TO USE `conda run` ---
    spectre_py_command = [
        "conda", "run", "-n", "spectre", "python", "./pipes/spectre.py",
        "--input-dir", to_dir,
        "--ref-genome", ref,
        "--log-base-dir", to_dir
        # Optional: Add --threads if you want to control mosdepth threads from pipeline.py
        # "--threads", "16"
    ]

    logger.info("Executing Command:\n" + ' '.join(spectre_py_command))
    try:
        spectre_result = subprocess.run(spectre_py_command, check=True, capture_output=True, text=True)
        # Log output from spectre.py within pipeline.log for context
        if spectre_result.stdout:
            logger.info(f"spectre.py stdout:\n{spectre_result.stdout.strip()}")
        if spectre_result.stderr:
            logger.warning(f"spectre.py stderr:\n{spectre_result.stderr.strip()}")
        logger.info("spectre.py executed successfully.")
    except subprocess.CalledProcessError as e:
        logger.error(f"Execution of spectre.py failed with exit code {e.returncode}")
        if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
        if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
        # Decide if you want to exit pipeline.py here
        # sys.exit(f"Spectre step failed.")
    except Exception as e:
        logger.exception("An unexpected error occurred during the Spectre step:")
        # Decide if you want to exit
        # sys.exit("Unexpected error in Spectre step.")


    logger.info("%%%%%%%%%%%%%\n FINISHED SPECTRE \n%%%%%%%%%%%%%")
    logger.info('Next Step: SNP Annotation')
    #----------------------

    # Step 5: SNP Annotation on .phased.vcf file

        # Define paths and prefixes
    deepvariant_dir = os.path.join(to_dir, "deepvariant") # Use os.path.join for consistency
    gz = f"{sample_name}_deepvariant_phased.vcf.gz"
    vcf_unzipped_relative = f"{sample_name}_deepvariant_phased.vcf" # Relative name
    out_snpeff = f"{sample_name}_deepvariant.phased.snpeff" # Prefix for output

    vcf_unzipped_abs_path = os.path.abspath(os.path.join(deepvariant_dir, vcf_unzipped_relative))

    # Define expected location of snpEff (adjust if different)
    # Using the path revealed in your previous error log:
    snpeff_base_dir = "/data/ref/snpEff"
    snpeff_jar_path = os.path.join(snpeff_base_dir, 'snpEff.jar')

    # Check if snpEff.jar exists before attempting to run
    if os.path.exists(snpeff_jar_path):
        logger.info(f"Running snpEff annotation for {sample_name} using snpeff.py")
        logger.info(f"Using absolute path for VCF: {vcf_unzipped_abs_path}") # Log the path being used

        # Construct the command to run snpeff.py
        snpeff_py_command = [
            "python", "./pipes/snpeff.py",
            "--snp-indel-dir", deepvariant_dir, # Pass the directory path
            "--vcf-gz", gz,                   # Pass the gzipped filename
            "--vcf-unzipped", vcf_unzipped_relative,
            "--out-prefix", out_snpeff,
            "--log-base-dir", to_dir,
            "--snpeff-base-dir", snpeff_base_dir # Pass the correct snpEff location
        ]

        # Log the command being run
        logger.info("Executing Command:\n" + ' '.join(snpeff_py_command))

        # Execute the snpeff.py script
        try:
            result_snpeff_py = subprocess.run(snpeff_py_command, check=True, capture_output=True, text=True)

            if result_snpeff_py.stdout:
                logger.info(f"snpeff.py stdout:\n{result_snpeff_py.stdout.strip()}")
            if result_snpeff_py.stderr:
                logger.warning(f"snpeff.py stderr:\n{result_snpeff_py.stderr.strip()}")
            logger.info("snpeff.py script executed successfully.")

            # Define the expected final output file (adjust if dbSNP step is added back)
            final_annotated_vcf = os.path.join(deepvariant_dir, f"{out_snpeff}.clinvar.vcf")
            if not os.path.exists(final_annotated_vcf):
                 logger.warning(f"Expected final annotated VCF not found after snpeff.py run: {final_annotated_vcf}")

        except subprocess.CalledProcessError as e:
            logger.error(f"Execution of snpeff.py failed with exit code {e.returncode}")
            if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
            if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
            # Decide if you want to exit the pipeline here
            # sys.exit(f"SnpEff step failed with exit code {e.returncode}")
        except Exception as e:
             logger.exception("An unexpected error occurred during the SnpEff step:")
             # Decide if you want to exit
             # sys.exit("Unexpected error in SnpEff step.")

    else:
        logger.error(f"snpEff.jar not found at {snpeff_jar_path}. Skipping SnpEff annotation.")
        # Decide if you want to exit or just continue
        # sys.exit("SnpEff dependency missing.")

    logger.info("%%%%%%%%%%%%%\n FINISHED SNP ANNOTATION \n%%%%%%%%%%%%%")
    logger.info('Next Step: SV-Calling via SNIFFLES2')




    #----------------------
    # Step 6: SV-Calling via SNIFFLES2

     # Define input BAM (Assuming haplotagged output from whatshap is desired)
    deepvariant_dir = os.path.join(to_dir, "deepvariant")
    # New haplotagged BAM from whatshap
    input_bam_sniffles = os.path.join(deepvariant_dir, f"{sample_name}_deepvariant_haplotagged.bam")

    # Define output VCF path
    vcf_name_sniffles = os.path.join(to_dir, f"{sample_name}.sniffles.vcf")
    sample_id = sample_name # Redundant, just for clarity

    # Check if the selected input BAM exists
    if os.path.exists(input_bam_sniffles):
        logger.info(f"Running Sniffles2 on {sample_name} using sniffles2.py")

        # Define timeout (example: 6 hours = 21600 seconds)
        # Adjust as needed based on typical run times
        sniffles_timeout_seconds = 21600

        # Construct the command to run sniffles2.py
        sniffles2_py_command = [
            "conda", "run", "-n", "sniffles",
            "python", "./pipes/sniffles2.py",
            "--input-bam", input_bam_sniffles,
            "--output-vcf", vcf_name_sniffles,
            "--ref", ref,
            "--sample-id", sample_id,
            "--log-base-dir", to_dir,
            "--threads", "16", # Or make this configurable
            "--timeout", str(sniffles_timeout_seconds) # Pass timeout to the script
        ]

        logger.info("Executing Command:\n" + ' '.join(sniffles2_py_command))

        try:
            # Run sniffles2.py. It handles its own timeout logging now.
            result_sniffles2_py = subprocess.run(sniffles2_py_command, check=True, capture_output=True, text=True)

            # Log output from sniffles2.py within pipeline.log for context
            if result_sniffles2_py.stdout:
                logger.info(f"sniffles2.py stdout:\n{result_sniffles2_py.stdout.strip()}")
            if result_sniffles2_py.stderr:
                logger.warning(f"sniffles2.py stderr:\n{result_sniffles2_py.stderr.strip()}")
            logger.info("sniffles2.py executed successfully.")

        except subprocess.CalledProcessError as e:
            # Check if the error code indicates a timeout occurred inside sniffles2.py (exit code 1 in the script)
            if e.returncode == 1 and "Sniffles2 command timed out" in e.stderr: # Check stderr for timeout message
                 logger.warning("Sniffles2 timed out (as reported by sniffles2.py). Proceeding with cleanup.")
                 # Don't necessarily exit pipeline.py here, allow cleanup.
            else:
                 logger.error(f"Execution of sniffles2.py failed with exit code {e.returncode}")
                 if e.stdout: logger.error(f"Failed command stdout:\n{e.stdout.strip()}")
                 if e.stderr: logger.error(f"Failed command stderr:\n{e.stderr.strip()}")
                 # Decide if you want to exit the pipeline here
                 # sys.exit(f"Sniffles2 step failed.")
        except Exception as e:
             logger.exception("An unexpected error occurred during the Sniffles2 step:")
             # Decide if you want to exit
             # sys.exit("Unexpected error in Sniffles2 step.")

    else:
        logger.error(f"Input BAM file for Sniffles2 not found: {input_bam_sniffles}. Skipping Sniffles2.")
        # Decide if you want to exit here
        # sys.exit("Input BAM for Sniffles2 not found.")

    logger.info("%%%%%%%%%%%%%\n FINISHED SNIFFLES2 (or timed out) \n%%%%%%%%%%%%%")

    # --- Keep the cleanup step ---
    logger.info("Attempting to clean up any lingering Sniffles processes...")
    # Use run with capture_output=True to avoid printing errors if no process is found
    cleanup_result = subprocess.run("pgrep -x sniffles | xargs kill -9", shell=True, capture_output=True, text=True)
    if cleanup_result.returncode == 0: # pgrep found something
        logger.info("Kill command sent to potential lingering Sniffles processes.")
    elif "kill: sending signal to XXX failed: No such process" in cleanup_result.stderr or cleanup_result.returncode != 0:
         logger.info("No lingering Sniffles processes found by pgrep or kill failed (may be expected if timeout didn't occur or process exited).")
    else:
         logger.warning(f"Sniffles cleanup command stderr: {cleanup_result.stderr.strip()}")

    logger.info("Pipeline Complete.") # Assuming this is the last step

if **name** == '**main**':
main()

{% endhighlight %}
